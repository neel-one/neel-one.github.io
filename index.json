[{"authors":null,"categories":null,"content":"Hello! I am a Junior at the University of Michigan, studying computer science, data science and math. I have worked on machine learning based image reconstruction research (MIRTorch) and led a game reinforcement learning project for Michigan Data Science Team. Last summer, I interned within the AI/ML division at Apple, working in the Machine Learning Platform and Technology group.\nOutside of my academic and professional interests and experiences, I can solve a Rubik\u0026rsquo;s cube in 10$\\pm$2 seconds, have started writing more (posts), try to keep up with my fitness goals (just bought a new bike), and enjoy a game or two of Valorant or The Last of Us 2.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c5bc44517c8bf39e10459d11db47c43b","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello! I am a Junior at the University of Michigan, studying computer science, data science and math. I have worked on machine learning based image reconstruction research (MIRTorch) and led a game reinforcement learning project for Michigan Data Science Team.","tags":null,"title":"Neel Shah","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://www.neelsh.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Neel Shah"],"categories":[],"content":"Intro During the Winter 2021 semester, I decided to lead a project team within Michigan Data Science Team. Overall, I think the project was successful; each team generated interesting results. Many enjoyed the project, tackled each work session with dedication, and finished the project with newfound knowledge. This experience was highly valuable to me and taught me to become a better project lead.\nUsually, MDST projects follow a traditional \u0026lsquo;data science\u0026rsquo; workflow: develop an objective, create/find a meaningful dataset, understand the data, build a model, analyze model\u0026rsquo;s performance on objective, and iterate.\nI opted to try a non-traditional, exploratory topic: Reinforcement Learning. Specifically, I tasked my team to try to create AI for a game called Pokemon Showdown, which I will call Showdown for the rest of this post.\nMotivation Quite a few reasons compelled me to choose Showdown over other games. First, Showdown is a fairly popular online game with around 15,000 users online at a given time. We can also test AI agents against real people using the ELO system. Second, Showdown is an open, unstudied game unlike chess, tetris, or poker. I didn\u0026rsquo;t want students copying from other literature to encourage fresh ideas unique to Showdown. Third, Showdown is open source and a Python library exists to interact with the website (both prod and local versions) to create AI agents. Lastly, Showdown is a zero-sum two-player imperfect information game. What does this mean? Showdown games have two players where only one wins. Players in a Showdown game do not have knowledge of the full state of the game (e.g. a player doesn\u0026rsquo;t know data about their opponent\u0026rsquo;s Pokemon). The search space of each game is effectively infinite with hidden branches (each move has an element of randomness associated with it, and we do not know which moves the opponent has), so a tree search based algorithm such as MCTS in AlphaGo doesn\u0026rsquo;t quite make sense. I liked the idea of a balance between chess (two-player, zero-sum) and poker (imperfect information). Regarding project feasibility, I felt that (without proof) Showdown RL algorithms have a much lower computational necessity than say the $35 billion price tag of AlphaGo. I am still not sure about the last point, but I can say for a fact that using Google Colab\u0026rsquo;s free resources is definitely not enough!\nTeam PokeRL had around 10 members, ranging from freshmen who had just learned to code to a PhD student with published research and graduate machine learning courses under his belt. As I hoped for each student to have a meaningful project experience while learning knew material, setting expectations and developing educational material/lectures was challenging for me.\nI surveyed each member and partitioned teams based on interest, experience, and algorithms that they wished to explore. I challenged teams to create the best AI that they could to be evaluated against other teams on the last session - gameday. Three teams were formed, self-named Elite4, Virn, and Solo Pokemon. I considered making my own team, but I mistakenly underestimated the time overhead of running the project, helping teams debug, discussing algorithmic details/implementation, and teaching general software/machine learning skills.\nWe met weekly on Sunday from 11am-2pm for seven weeks during the semester. Initially, I thought to split up each session with a 1 hour combined lecture and a 2 hour team work period. However, by the third session, I decided to shorten or cut out the lecture. I realized that the experience level of the members were on the lower end with most members fairly new programmers and with not as many math/cs courses under their belts. Once I understood this, I decided to shorten each lecture and focus on less technical details. I spent the gained time talking to individual groups and teaching/explaining a topic that they were interested in or were struggling to apply to their project. For students with more experience or interest in technical details, I individually explored relevant topics.\nSet Up To kickstart each team, I provided each team with starter code with instructions to run and a greedy agent implementation, a simple DQN agent implementation (from the official documentation), and a tf/keras modelled player interface.\nInitially, I anticipated members to run their code locally. Instead, some were met with technical difficulties. As some of those people were less experienced, we lost valuable project session time helping them debug. I felt bad - the inability to set up is frustrating, and the student would feel less motivated, especially during a long three hour work session. This mistake on my part cost us limited productivity hours before I decided to shift everything to Google Colab. The migration was not so simple because I had to figure out how to run a local server within Colab (games are player through a socket connection to a local Showdown server). These issues would have never happened if I had been more proactive in my project preparation.\nAlgorithms As a baseline model, I created a greedy agent who is (almost) guaranteed to make a move that deals the most damage to the opponent\u0026rsquo;s current Pokemon. The greedy agent hovers slightly above 1000 ELO, the default player ELO, when deployed to the official Showdown server.\nSolo Pokemon and Virn developed unique heuristic, rule-based algorithms. Elite4 decided to try a neural network powered reinforcement learning algorithm.\nSolo Pokemon Solo Pokemon\u0026rsquo;s algorithm attempted to determine whether to play a greedy move, switch to another Pokemon (at the beginning of each game, players start with six Pokemon), or dynamax (a feature unique to the specific game-mode in Showdown). If the current Pokemon had a unfavorable speed, a type disadvantage, and high probability of fainting (when the Pokemon is unusable for the rest of the game) based on its current health and defense stats, Solo Pokemon opted to switch into another Pokemon. Dynamax was determined based on the alive Pokemon in their party, a strong type advantage, and the base stats of the current Pokemon. In all other cases, Solo Pokemon chose a greedy move.\nVirn Virn had a simple but effective approach. If the current Pokemon had the best move against the opponent Pokemon, then the current Pokemon would use that move, dynamaxing if the move was super affective (favorable type advantage). Otherwise, switch to the Pokemon on the bench with the best move.\nI think it is interesting to note that Solo Pokemon and Virn calculated the best move differently, although I won\u0026rsquo;t go into specific details as they require a bit more knowledge about the game.\nElite4 Elite4 used Deep Q-Learning with Experience Replay with Double DQN. More information about exact details can be found here: DQN paper, Double DQN paper, library implementation. This algorithm is fairly well documented and implemented, allowing the team to spend their time to experiment state vectorization, network architecture, reward calculation, opponent selection, and more. I doubt DQN is near an optimal approach to this problem, but I think DQN is a very solid choice of algorithm for a weekly, semester long team project #with less experienced students. I found it easier to build an intuition for this algorithm for those interested. Elite4 experimented with different ways to represent state based off the data given from the environment, settling on a state vector with 21 components combining Pokemon types, move powers and multipliers, statuses, and dynamax ability. They experimented with ways to represent reward and model architectures, settling on one with 7000 parameters. The model\u0026rsquo;s input is a state array and output is an array of action probabilities for 22 possible actions. I do not recall the exact values they used for parameters such as $\\epsilon$ for the $\\epsilon$-greedy policy, experience replay memory size, $\\gamma$ for reward discount factor, among other tunable parameters.\nTo train, Elite4 first trained their model through battles against the greedy agent until they had a ~60% win rate. Then, Elite4 trained against an opponent with a fixed snapshot of its model, periodically updating the opponent\u0026rsquo;s model if the current model consistently beat the opponent. The team attempted to train on Colab, but their methods were still too time consuming and computationally intensive for reaching satisfactory results. Nevertheless, they attained a model trained for about 15 hours.\nResults Against the greedy agent, Solo Pokemon and Virn claimed a 75% win rate, while Elite4 had a 65% win rate.\nThe table below shows results of a round robin tournament between the three teams.\nGames won out of 1000; row represents challenger and column represents opponent\n    Solo Pokemon Virn Elite4     Solo Pokemon - 451 596   Virn 549 - 656   Elite4 404 344 -    I thought it was cool battles were quite fair - no team was completely wiped/wiped other teams. I was suprised by Elite4\u0026rsquo;s relatively formidable results, despite the difficulties in training and challenges that the team faced throughout the semester. With current approaches, Virn is the winner of MDST PokeRL with 55% win rate over Solo Pokemon and a 65% win rate over Elite4!\nBelow you can watch one battle between each team.\n   From a human perspective, we can see mistakes that agents are making. These mistakes are generally easier to fix for heuristic based agents but not so simple for an RL trained agent. Further Work I mentioned above that I was also interested in creating my own RL agent with a more sophisticated model and algorithm selection and training based on my knowledge. Although I did not have time in the semester due to my position, I decided to tackle this problem afterwards. Due to my internship I didn\u0026rsquo;t have much time to work, but I am gaining progress in my approach. My current results are not in a presentable state, but I will update this post in the future with results. I aim to get as good of an AI as possible with a solid ELO rating and acceptable performance against most active users on Showdown.\nIf you want updates, feel free to contact me and ask!\n","date":1629773175,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629773175,"objectID":"e85a169fcd445686e8ee2bcd0a562a48","permalink":"https://www.neelsh.com/post/reflections-pokerl/","publishdate":"2021-08-23T22:46:15-04:00","relpermalink":"/post/reflections-pokerl/","section":"post","summary":"Pokemon Reinforcement Learning: thoughts from a project leader perspective","tags":[],"title":"Creating Pokemon AI","type":"post"},{"authors":["Neel Shah"],"categories":[],"content":"In May 2018, I was wrapping up my junior year in high school with not much to do in the summer. At that time, I was particularly interested in materials science and mechanical engineering and thought I wanted to pursue a career in one of those fields. I also had an slight interest in research having worked on two Science Fair projects and taken a research techniques mini-course. I decided to approach some mechanical engineering professors at a nearby University with those two interests in mind.\nMost professors I contacted ghosted me, but one decided to meet with me. He mentioned two potential topics of interest: chemical combustion graph analysis and machine learning based predictive models for combustion properties. As I had no knowledge of any part of either topic, I spent the next few weeks reviewing materials and looking through a set of 10 papers that he sent me.\nOne paper, Planning chemical syntheses with deep neural networks and symbolic AI by Segler et al., particularly caught my eye. I spent near a week trying to understand the paper with no success whatsoever. But the mountains of information I came across while learning piqued my curiosity in machine learning as a lot of technical details aligned with my academic skills and interests. Fun fact: the creator of one site that I printed out Rubik\u0026rsquo;s cube techniques for back in 2011 happened to teach a popular deep learning course, CS231n, and now works on cool autopilot stuff at Tesla.\nWe decided to demonstrate accurate predictive models for autoignition and flame properties using machine learning methods. Specifically, I used random forests and neural networks on various datasets to predict ignition delay times, laminar flame speeds, octane ratings, and CA50 values in HCCI engines. We believed machine learning models can alleviate long-held limitations from traditional empirical models. For example, the chemical kinetics and resultant intermediaries of some reactions can change drastically based on pressure and temperature.\nI spent the summer reading and understanding relevant papers, writing code, running experiments, and presenting to the research group. Don\u0026rsquo;t worry, I still had fun hanging out with friends and family as a high schooler should!\nDuring the school year, I refined a write-up that was accepted to SAE World Congress Experience 2019 (SAE WCX 2019). From April 9-11th, 2019, I missed my school mornings to attend the conference, network with, and see talks and demos from industry leaders. The experience was surreal. On the day of my talk, I came up sick and was losing my voice. Before my presentation begun, I began to feel slightly nervous. I had lost half of my voice in front of a crowd of around 70 people. I had doubts about my knowledge: what can a high schooler teach to industry veterans or people with graduate degrees in this field?\nThe talk itself was a blur. Once I began, I boldly shared my presentation that I had spent so long preparing with work that I had known for 11 months (with 3 months of focused work). Afterwards, I fielded questions. I don\u0026rsquo;t remember that part now, but I do recall some questions being quite difficult to answer. I know for sure that I did answer them to the best of my ability. I think the talk was 20 minutes long with 10 minutes of questions, but it may have been longer; I honestly can\u0026rsquo;t remember now over two years later.\nThis project was a great first University level research experience for me. I learned that I was capable of picking up new information and generating valuable work even without a traditional background (e.g. degree or course experience). I picked up the skill of reading and understanding academic papers. I got to experience an academic conference as a presenter. I overcame many technical problems during my work - even identifying a mistake in the paper for one state-of-the-art empircal model.\nThis webpage has a picture of me after my talk and this ResearchGate link goes to my conference published paper which currently seems to have 7 citations!\n","date":1621022728,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621022728,"objectID":"665f4fa14aabae3fd741ff58db94235d","permalink":"https://www.neelsh.com/post/first-university-research/","publishdate":"2021-05-14T16:05:28-04:00","relpermalink":"/post/first-university-research/","section":"post","summary":"The story behind my first published work","tags":[],"title":"First University Research Project","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://www.neelsh.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://www.neelsh.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ce6f3bce504c23757ffb63738465b3b0","permalink":"https://www.neelsh.com/project/comingsoon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/comingsoon/","section":"project","summary":"","tags":null,"title":"Coming Soon","type":"project"}]