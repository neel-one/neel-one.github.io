<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Neel Shah</title>
    <link>https://www.neelsh.com/post/</link>
      <atom:link href="https://www.neelsh.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2023 Neel Shah</copyright><lastBuildDate>Tue, 11 Jul 2023 21:11:57 -0400</lastBuildDate>
    <image>
      <url>https://www.neelsh.com/media/icon_hu9eb7868fa6d35fd04542d78ce5349143_23965_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://www.neelsh.com/post/</link>
    </image>
    
    <item>
      <title>NOCAP: A not-so-useful precompilation framework</title>
      <link>https://www.neelsh.com/post/nocap/</link>
      <pubDate>Tue, 11 Jul 2023 21:11:57 -0400</pubDate>
      <guid>https://www.neelsh.com/post/nocap/</guid>
      <description>&lt;p&gt;I deeply enjoyed Advanced Compilers - my final technical course in university and my first dabble into formally studying compilers. For the final project, I teamed up with classmates to build a compilers or compilers-adjacent research project.&lt;/p&gt;
&lt;p&gt;Influenced by some ideas present in &lt;a href=&#34;https://dada.cs.washington.edu/research/tr/2015/01/UW-CSE-15-01-01.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this paper presenting an approximate computing framework&lt;/a&gt;, we sought to implement build ideas in approximate computing, a study that trades program accuracy for improvements in speed and/or memory usage. The quintessential example of approximate computing is loop perforation, which modifies the source to skip loop iterations. After surveying other relevant papers, we converged on a few actionable ideas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expand ACCEPT to contain more approximate computing techniques. Ironically, I could not compile the ACCEPT compiler framework after days of hacking away and squashing compiler bugs as they appeared (dependencies suck). That idea was out of question&lt;/li&gt;
&lt;li&gt;Implement alternate algorithm selection (such as using a less accurate but fast matrix multiplication algorithm in certain scenarios for image processing or machine learning training/inference) or alternate data structure selection (can we determine at compile time which implementation of a key-value store is best for the workload?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While whiteboarding through some ideas and potential timelines, two techniques lingered in my mind: small-angle approximation (the property that $\sin(x) \approxeq x$ when $x$ is small) and Taylor approximations (you can estimate continuous functions with some derivatives). We realized that, in general, if we assume that functions are continuous, we can approximate values quite easily. The accuracy of the approximations depends on the function&amp;rsquo;s gradient and the point around which we find the approximation. A flat horizontal line is the perfect case. We decided to maximize speed by assigning a fixed value to each input interval - equivalent to a 0th order Taylor approximation.&lt;/p&gt;
&lt;p&gt;Thus Nearby-operand Continuous Approximation (NOCAP) was borne. The user annotates candidate double -&amp;gt; double functions to annotate. The NOCAP profiler identifies a range of inputs based on sample input. Then, with user-specified memory usage, NOCAP replaces function calls to candidate functions with table queries, falling back to the original if the input is out of range. NOCAP trades accuracy and memory usage for speed.&lt;/p&gt;
&lt;p&gt;We used two benchmarks - a toy example using the built-in &lt;code&gt;exp()&lt;/code&gt; function in &lt;code&gt;math.h&lt;/code&gt; - and an implementation of Black-Scholes that uses &lt;code&gt;exp(), log(), sqrt()&lt;/code&gt; in &lt;code&gt;math.h&lt;/code&gt;. In the toy example, NOCAP shows a 60% speedup for 200 million calls to exp() but less than a 1% speedup for Black-Scholes. These speedups are statistically verified. NOCAP demonstrates speedup but in code where math functions are such a small part of CPU usage, the speedup is negligible. Exp is also a bad example to use NOCAP outside of demonstrating speedup because of accuracy losses resulting from the very large and very small gradients at both ends of the input spectrum.&lt;/p&gt;
&lt;p&gt;However, table lookup could suffer a bottleneck to cache size/RAM size.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get ready for some napkin math&amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;napkin-math&#34;&gt;Napkin math&lt;/h1&gt;
&lt;p&gt;Based on ChatGPT and a Google/Quora/Stack Overflow for &amp;ldquo;modern processors&amp;rdquo; (I can&amp;rsquo;t be bothered to keep track of sources for napkin math), addition/subtraction/multiplication/L1 cache hit (~1-2 cycles) &amp;lt;= division/L3 cache hit (~30 cycles) &amp;lt;= local SRAM/DRAM (~100 cycles or more) (SRAM is faster). Are these numbers totally accurate? Definitely not, but I think we can see a general pattern arise. It takes a lot of arithmetic to be slower than RAM access but not that much to be slower than an L3 cache hit.&lt;/p&gt;
&lt;p&gt;Suppose we have some modern hardware with a relatively large L3 cache - 32MB. NOCAP can accommodate a max of 4 million table entries. With a .01 interval size, we can only accommodate a max range of 40,000. Instead, if we &lt;a href=&#34;https://www.amd.com/en/products/cpu/amd-epyc-9684x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spend $15,000&lt;/a&gt;, we can get a 1.1GB L3 cache, giving us a max range of ~1.3 million - a bit better!&lt;/p&gt;
&lt;p&gt;Maybe the function is really slow, and using RAM is worth it. 64GB of RAM affords you ~83 million range and 1TB of RAM affords you ~1.3 billion range - a lot better!&lt;/p&gt;
&lt;p&gt;Note that the ranges aren&amp;rsquo;t quite correct because doubles are stored using the IEEE floating point technique.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;NOCAP was a fun project to work on and maybe the ideas presented are useful for a specific use case. However, I don&amp;rsquo;t think NOCAP is useful for 99.999999% of continuous functions, especially if only arithmetic is used to compute it (e.g. no blocking operations or syscalls). Perhaps it&amp;rsquo;s better than a lot of division but I have not verified that claim. Practically, L3 cache hits are the most frequent NOCAP queries, and the L3 cache is almost always too small for any reasonable range of inputs.&lt;/p&gt;
&lt;p&gt;Enjoy demo slides and our paper on the project.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;nocap_slides.pdf&#34;&gt;Demo Slides&lt;/a&gt;&lt;/p&gt;
&lt;iframe src=&#34;nocap_slides.pdf&#34; width=&#34;100%&#34; height=&#34;500px&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&#34;nocap_report.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;iframe src=&#34;nocap_report.pdf&#34; width=&#34;100%&#34; height=&#34;500px&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Creating Pokemon AI</title>
      <link>https://www.neelsh.com/post/reflections-pokerl/</link>
      <pubDate>Mon, 23 Aug 2021 22:46:15 -0400</pubDate>
      <guid>https://www.neelsh.com/post/reflections-pokerl/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;
&lt;p&gt;During the Winter 2021 semester, I decided to lead a project team within &lt;a href=&#34;https://www.mdst.club/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michigan Data Science Team&lt;/a&gt;. Overall, I think the project was successful; each team generated interesting results. Many enjoyed the project, tackled each work session with dedication, and finished the project with newfound knowledge. Leading PokeRL will resonate with me for years to come.&lt;/p&gt;
&lt;p&gt;Usually, MDST projects follow a traditional &amp;lsquo;data science&amp;rsquo; workflow: develop an objective, create/find a meaningful dataset, understand the data, build a model, analyze the model&amp;rsquo;s performance on objective, and iterate.&lt;/p&gt;
&lt;p&gt;I opted to try a non-traditional, exploratory topic: Reinforcement Learning. Specifically, I tasked my team to try to create AI for a game called &lt;a href=&#34;https://pokemonshowdown.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pokemon Showdown&lt;/a&gt;, which I will call Showdown for the rest of this post.&lt;/p&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Quite a few reasons compelled me to choose Showdown over other games. First, Showdown is a fairly popular online game with around 15,000 users online at a given time. We can also test AI agents against real people using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Elo_rating_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ELO system&lt;/a&gt;. Second, Showdown is an open, unstudied game unlike chess, tetris, or poker. I didn&amp;rsquo;t want students copying from other literature to encourage fresh ideas unique to Showdown. Third, Showdown is open source and a &lt;a href=&#34;http://poke-env.readthedocs.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python library&lt;/a&gt; exists to interact with the website (both prod and local versions) to create AI agents. Lastly, Showdown is a zero-sum two-player imperfect information game. What does this mean? Showdown games have two players with one winner and no draws. Players in a Showdown game do not know the full state of the game (e.g. a player doesn&amp;rsquo;t know data about their opponent&amp;rsquo;s Pokemon). The search space of each game is effectively infinite with hidden branches (each move has an element of randomness associated with it, and we do not know which moves the opponent has), so a tree search based algorithm such as MCTS in AlphaGo doesn&amp;rsquo;t quite make sense. I liked the idea of a balance between chess (two-player, zero-sum) and poker (imperfect information). Regarding project feasibility, I felt that (without proof) Showdown RL algorithms have a much lower computational necessity than say the $35 billion price tag of AlphaGo. I am still not sure about the last point, but I can say for a fact that using Google Colab&amp;rsquo;s free resources is definitely &lt;strong&gt;not&lt;/strong&gt; enough!&lt;/p&gt;
&lt;h1 id=&#34;team&#34;&gt;Team&lt;/h1&gt;
&lt;p&gt;PokeRL had around 10 members, ranging from freshmen who had just learned to code to a PhD student with published research and graduate machine learning courses under his belt. As I hoped for each student to have a meaningful project experience while learning new material, setting expectations and developing educational material/lectures was challenging for me.&lt;/p&gt;
&lt;p&gt;I surveyed each member and partitioned teams based on interest, experience, and algorithms that they wished to explore. I challenged teams to create the best AI possible to be evaluated against other teams on &lt;em&gt;gameday&lt;/em&gt;. Three teams were formed, self-named Elite4, Virn, and Solo Pokemon. I considered making my own team, but I mistakenly underestimated the time overhead of running the project, helping teams debug, discussing algorithmic details/implementation, and teaching general software/machine learning skills.&lt;/p&gt;
&lt;p&gt;We met weekly on Sunday from 11am-2pm for seven weeks during the semester. Initially, I thought to split up each session with a 1 hour combined lecture and a 2 hour team work period. However, by the third session, I decided to shorten or cut out the lecture.
Personalized focus time between each group provided a more valuable use of time. Groups varied in technical experience, and focus time allowed me to tailor my help individually. For advanced members, discussions replaced help.&lt;/p&gt;
&lt;h1 id=&#34;set-up&#34;&gt;Set Up&lt;/h1&gt;
&lt;p&gt;To kickstart each team, I provided each team with starter code with instructions to run and a greedy agent implementation, a simple DQN agent implementation (from the official documentation), and a tf/keras modeled player interface.&lt;/p&gt;
&lt;p&gt;Initially, I anticipated members to run their code locally. Instead, some were met with technical difficulties. As some of those people were less experienced, we lost valuable project session time helping them debug. I felt bad - the inability to set up is frustrating, and the student would feel less motivated, especially during a long three hour work session. This mistake on my part cost us limited productivity hours before I decided to shift everything to Google Colab. The migration was not so simple because I had to figure out how to run a local server within Colab (games are played through a socket connection to a local Showdown server). These issues would have never happened if I had been more proactive in my project preparation.&lt;/p&gt;
&lt;h1 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h1&gt;
&lt;p&gt;As a baseline model, I created a greedy agent who is (almost) guaranteed to make a move that deals the most damage to the opponent&amp;rsquo;s current Pokemon. The greedy agent hovers slightly above 1000 ELO, the default player ELO, when deployed to the official Showdown server.&lt;/p&gt;
&lt;p&gt;Solo Pokemon and Virn developed unique heuristic, rule-based algorithms. Elite4 decided to try a neural network powered reinforcement learning algorithm.&lt;/p&gt;
&lt;h3 id=&#34;solo-pokemon&#34;&gt;Solo Pokemon&lt;/h3&gt;
&lt;p&gt;Solo Pokemon&amp;rsquo;s algorithm attempted to determine whether to play a greedy move, switch to another Pokemon (at the beginning of each game, players start with six Pokemon), or dynamax (a feature unique to the specific game mode in Showdown). If the current Pokemon had an unfavorable speed, a type disadvantage, and a high probability of fainting (when the Pokemon is unusable for the rest of the game) based on its current health and defense stats, Solo Pokemon opted to switch to another Pokemon. Dynamax was determined based on the alive Pokemon in their party, a strong type advantage, and the base stats of the current Pokemon. In all other cases, Solo Pokemon chose a greedy move.&lt;/p&gt;
&lt;h3 id=&#34;virn&#34;&gt;Virn&lt;/h3&gt;
&lt;p&gt;Virn had a simple but effective approach. If the current Pokemon had the best move against the opponent Pokemon, then the current Pokemon would use that move, dynamaxing if the move was super effective (favorable type advantage). Otherwise, switch to the Pokemon on the bench with the best move.&lt;/p&gt;
&lt;p&gt;I think it is interesting to note that Solo Pokemon and Virn calculated the best move differently, although I won&amp;rsquo;t go into specific details as they require a bit more knowledge about the game.&lt;/p&gt;
&lt;h3 id=&#34;elite4&#34;&gt;Elite4&lt;/h3&gt;
&lt;p&gt;Elite4 used Deep Q-Learning with Experience Replay with Double DQN. More information about exact details can be found here: &lt;a href=&#34;https://arxiv.org/pdf/1312.5602.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DQN paper&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1509.06461.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Double DQN paper&lt;/a&gt;,  &lt;a href=&#34;https://github.com/keras-rl/keras-rl/blob/216c3145f3dc4d17877be26ca2185ce7db462bad/rl/agents/dqn.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;library implementation&lt;/a&gt;. This algorithm is fairly well documented and implemented, allowing the team to spend their time to experiment state vectorization, network architecture, reward calculation, opponent selection, and more. I doubt DQN is near an optimal approach to this problem, but I think DQN is a very solid choice of algorithm for a weekly, semester long team project #with less experienced students. I found it easier to build an intuition for this algorithm for those interested. Elite4 experimented with different ways to represent state based on the data given from the environment, settling on a state vector with 21 components combining Pokemon types, move powers and multipliers, statuses, and dynamax ability. They experimented with ways to represent reward and model architectures, settling on one with 7000 parameters. The model&amp;rsquo;s input is a state array, and its output is an array of action probabilities for 22 possible actions. I do not recall the exact values they used for parameters such as $\epsilon$ for the $\epsilon$-greedy policy, experience replay memory size, $\gamma$ for reward discount factor, among other tunable parameters.&lt;/p&gt;
&lt;p&gt;To train, Elite4 first trained their model through battles against the greedy agent until they had a ~60% win rate. Then, Elite4 trained against an opponent with a fixed snapshot of its model, periodically updating the opponent&amp;rsquo;s model if the current model consistently beat the opponent. The team attempted to train on Colab, but their methods were still too time consuming and computationally intensive for reaching satisfactory results. Nevertheless, they attained a model trained for about 15 hours.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;Against the greedy agent, Solo Pokemon and Virn claimed a 75% win rate, while Elite4 had a 65% win rate.&lt;/p&gt;
&lt;p&gt;The table below shows the results of a round robin tournament between the three teams.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Games won out of 1000; row represents challenger and column represents opponent&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Solo Pokemon&lt;/th&gt;
&lt;th&gt;Virn&lt;/th&gt;
&lt;th&gt;Elite4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Solo Pokemon&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;451&lt;/td&gt;
&lt;td&gt;596&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Virn&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;549&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;656&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Elite4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;404&lt;/td&gt;
&lt;td&gt;344&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Interestingly, battles were quite fair - no team was completely wiped/wiped other teams. I was suprised by Elite4&amp;rsquo;s relatively formidable results, despite the difficulties in training and challenges that the team faced throughout the semester.
With current approaches, Virn is the winner of MDST PokeRL with 55% win rate over Solo Pokemon and a 65% win rate over Elite4!&lt;/p&gt;
&lt;p&gt;Below you can watch one battle between each team.&lt;/p&gt;
&lt;iframe 
src=&#34;Gen8RandomBattle-2021-04-10-teamsolopokemon-elite4rl copy.html&#34; width=&#34;100%&#34; height=&#34;500px&#34;&gt;
&lt;/iframe&gt;
&lt;iframe 
src=&#34;Gen8RandomBattle-2021-04-10-teamsolopokemon-virnstrategy.html&#34; width=&#34;100%&#34; height=&#34;500px&#34;&gt;
&lt;/iframe&gt;
&lt;iframe 
src=&#34;Gen8RandomBattle-2021-04-10-virnstrategy-elite4rl-2.html&#34; width=&#34;100%&#34; height=&#34;500px&#34;&gt;
&lt;/iframe&gt;
From a human perspective, we can see the mistakes that agents are making. These mistakes are generally easier to fix for heuristic based agents but not so simple for an RL trained agent.
&lt;h1 id=&#34;further-work&#34;&gt;Further Work&lt;/h1&gt;
&lt;p&gt;I mentioned above that I was also interested in creating my own RL agent with a more sophisticated model and algorithm selection and training based on my knowledge. Although I did not have time in the semester due to my position, I decided to tackle this problem afterward. Due to my internship, I didn&amp;rsquo;t have much time to work, but I am gaining progress in my approach. My current results are not in a presentable state, but I will update this post in the future with results. I aim to get as good of an AI as possible with a solid ELO rating and acceptable performance against most active users on Showdown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2022 Update:&lt;/strong&gt;
I ported state and network model representations to pytorch. I then used a library called &lt;code&gt;mushroom_rl&lt;/code&gt; and implemented a training loop with it. I spun up an AWS GPU instance to put all my hard work together. Exciting stuff! A few hours into the training run, I check in anddddddddddddddddddddddddddd the code is hanging. Hmm, what&amp;rsquo;s wrong?!? I diagnose the problem as an issue in battle simulation and after further investigation, I find a concurrency bug. I thought about this bug on and off for a long, long time but have no working solution. My priorities have changed and perhaps with full-time headbashing effort, I can engineer a solution. But for now, this quest is on pause.&lt;/p&gt;
&lt;p&gt;Either way, I don&amp;rsquo;t think reinforcement learning purely is the best way to approach this problem. Ideally, I&amp;rsquo;d collect a large game database (perhaps a good use of my scraping knowledge), pretrain an attention-based network, and add in search and/or self play based techniques at the very end.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How I published research in high school</title>
      <link>https://www.neelsh.com/post/first-university-research/</link>
      <pubDate>Fri, 14 May 2021 16:05:28 -0400</pubDate>
      <guid>https://www.neelsh.com/post/first-university-research/</guid>
      <description>&lt;p&gt;As my junior year of high school drew to a close in May 2018, a sense of emptiness loomed over the upcoming summer. My Science Fair project experience and an interest in materials science and mechanical engineering compelled me to seek research opportunities at nearby universities. I began scrolling faculty pages and cold emailing galore.&lt;/p&gt;
&lt;p&gt;To my dismay, nearly all professors I contacted ghosted me, but &lt;a href=&#34;https://mabe.utk.edu/people/peng-zhao/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one&lt;/a&gt; graciously agreed to meet me. Our conversation unveiled two research avenues: chemical combustion graph analysis and machine learning based predictive models for combustion properties. He shared ten papers with me, and I spent the next few weeks heads-deep in reading.&lt;/p&gt;
&lt;p&gt;One paper, &lt;a href=&#34;https://www.nature.com/articles/nature25978&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Planning chemical syntheses with deep neural networks and symbolic AI&lt;/a&gt; by Segler et al., particularly caught my eye in the sea of literature. I spent nearly a week trying to understand the paper with no success whatsoever. But the mountains of information I came across while learning piqued my curiosity in machine learning because the technical details aligned with my academic skills and interests. Fun fact: the creator of one &lt;a href=&#34;http://badmephisto.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;site&lt;/a&gt; that I printed out Rubik&amp;rsquo;s cube techniques for back in 2011 happened to teach a popular deep learning course, &lt;a href=&#34;http://cs231n.stanford.edu/2016/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n&lt;/a&gt; - the original implement of a deep learning course I took at UMich, and now works on &lt;a href=&#34;https://karpathy.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cool autopilot stuff at Tesla and intelligence at OpenAI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We decided to demonstrate accurate predictive models for autoignition and flame properties using machine learning methods. Specifically, I used random forests and neural networks on various datasets to predict ignition delay times, laminar flame speeds, octane ratings, and CA50 values in HCCI engines. We believed machine learning models can alleviate long-held limitations from traditional empirical models. For example, the chemical kinetics and resultant intermediaries of most reactions can change drastically based on pressure and temperature.&lt;/p&gt;
&lt;p&gt;I filled my summer reading and understanding relevant papers, writing code, running experiments, and presenting to the research group. Don&amp;rsquo;t worry, I still had fun hanging out with friends and family as a high schooler should!&lt;/p&gt;
&lt;p&gt;During the school year, I refined a write-up that was accepted to SAE World Congress Experience 2019 (SAE WCX 2019). From April 9-11th, 2019, I missed my school mornings to attend the conference, network with, and see talks and demos from industry leaders. The experience was surreal. On the day of my talk, I came up sick and was losing my voice. My stomach sank before the presentation began, and my heartbeats permeated across my body. I had lost half of my voice in front of a crowd of around 70 people. I had doubts about my knowledge: &lt;em&gt;what can a high schooler teach to industry veterans or people with graduate degrees in this field?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The talk itself was a blur. Once I began, I boldly shared my presentation that I had spent so long preparing with work that I had known for 11 months (with 3 months of focused work). Afterward, I fielded questions. I don&amp;rsquo;t remember that part now, but I do recall some questions being quite difficult to answer. I know for sure that I did answer them to the best of my ability. I think the talk was 20 minutes long with 10 minutes of questions, but it may have been longer; I honestly can&amp;rsquo;t remember now over two years later.&lt;/p&gt;
&lt;p&gt;This project was a great first University level research experience for me. I learned that I was capable of picking up new information and generating valuable work even without a traditional background (e.g. degree or course experience). I picked up the skill of reading and understanding academic papers. I experienced an academic conference as a presenter. I overcame many technical problems during my work - even identifying a mistake in the paper for one state-of-the-art empirical model.&lt;/p&gt;
&lt;p&gt;This &lt;a href=&#34;https://vtf-llc.com/2019/05/12/high-school-student-presented-sae-paper/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webpage&lt;/a&gt; has a picture of me after my talk and this &lt;a href=&#34;https://www.researchgate.net/publication/331966092_Prediction_of_Autoignition_and_Flame_Properties_for_Multicomponent_Fuels_Using_Machine_Learning_Techniques&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResearchGate link&lt;/a&gt; goes to my conference published paper which currently seems to have 7 citations!&lt;/p&gt;
&lt;p&gt;I had a short research stint in university. After significant consideration, I decided to put aside my research aspirations for other professional interests. The incentive structure and pace of a business align better with my disposition. I still skim a research paper here and there ðŸ˜Š.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
